{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca34a69-92da-401c-8439-4baf6d8c87a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass DecisionTree:\\n    def __init__(self, max_depth=None, min_samples_split=2):\\n        self.max_depth = max_depth\\n        self.min_samples_split = min_samples_split\\n        self.tree = None\\n\\n    def fit(self, X, y):\\n        self.tree = self._build_tree(X, y)\\n\\n    def predict(self, X):\\n        return np.array([self._traverse_tree(x, self.tree) for x in X])\\n\\n    def _gini(self, y):\\n        gini = 1.0\\n        for i in [0,1]:\\n            p = len(y[y == i]) / len(y)\\n            gini -= p**2\\n        return gini\\n\\n    def _split(self, X, y, feature, threshold):\\n        left_mask = X[:, feature] <= threshold\\n        right_mask = X[:, feature] > threshold\\n        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\\n\\n    def _best_split(self, X, y):\\n        best_gini = float(\\'inf\\')\\n        best_split = None\\n        for feature in range(X.shape[1]):\\n            thresholds = np.unique(X[:, feature])\\n            for threshold in thresholds:\\n                X_left, X_right, y_left, y_right = self._split(X, y, feature, threshold)\\n                if len(y_left) == 0 or len(y_right) == 0:\\n                    continue\\n\\n                gini_left = self._gini(y_left)\\n                gini_right = self._gini(y_right)\\n                gini = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right\\n\\n                if gini < best_gini:\\n                    best_gini = gini\\n                    best_split = {\\n                        \\'feature\\': feature,\\n                        \\'threshold\\': threshold,\\n                        \\'X_left\\': X_left,\\n                        \\'X_right\\': X_right,\\n                        \\'y_left\\': y_left,\\n                        \\'y_right\\': y_right\\n                    }\\n        return best_split\\n\\n    def _build_tree(self, X, y, depth=0):\\n        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\\n        predicted_class = np.argmax(num_samples_per_class)\\n        node = {\\n            \\'depth\\': depth,\\n            \\'num_samples\\': len(y),\\n            \\'num_samples_per_class\\': num_samples_per_class,\\n            \\'predicted_class\\': predicted_class,\\n        }\\n\\n        if depth < self.max_depth and len(y) >= self.min_samples_split:\\n            best_split = self._best_split(X, y)\\n            if best_split:\\n                node[\\'feature\\'] = best_split[\\'feature\\']\\n                node[\\'threshold\\'] = best_split[\\'threshold\\']\\n                node[\\'left\\'] = self._build_tree(best_split[\\'X_left\\'], best_split[\\'y_left\\'], depth + 1)\\n                node[\\'right\\'] = self._build_tree(best_split[\\'X_right\\'], best_split[\\'y_right\\'], depth + 1)\\n        return node\\n\\n    def _traverse_tree(self, x, node):\\n        if \\'threshold\\' in node:\\n            if x[node[\\'feature\\']] <= node[\\'threshold\\']:\\n                return self._traverse_tree(x, node[\\'left\\'])\\n            else:\\n                return self._traverse_tree(x, node[\\'right\\'])\\n        else:\\n            return node[\\'predicted_class\\']\\n\\nclass RandomForest:\\n    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2):\\n        self.n_estimators = n_estimators\\n        self.max_depth = max_depth\\n        self.min_samples_split = min_samples_split\\n        self.trees = []\\n\\n    def fit(self, X, y):\\n        self.trees = []\\n        for _ in range(self.n_estimators):\\n            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\\n            X_sample, y_sample = self._bootstrap_sample(X, y)\\n            tree.fit(X_sample, y_sample)\\n            self.trees.append(tree)\\n\\n    def predict(self, X):\\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\\n        return np.array([np.bincount(tree_preds[:, i]).argmax() for i in range(X.shape[0])])\\n\\n    def _bootstrap_sample(self, X, y):\\n        n_samples = X.shape[0]\\n        indices = np.random.choice(n_samples, n_samples, replace=True)\\n        return X[indices], y[indices]\\n\\n\\n# Example usage with churn dataset\\n\\n# Load the dataset\\ndf = pd.read_csv(\\'dataset.csv\\')\\ndf.dropna(inplace=True)\\n\\n# Encoding categorical features\\nlabel_encoders = {}\\nfor column in df.select_dtypes(include=[\\'object\\']).columns:\\n    label_encoders[column] = LabelEncoder()\\n    df[column] = label_encoders[column].fit_transform(df[column])\\n    \\n\\nX = df.drop(columns=[\\'Churn\\'])\\ny = df[\\'Churn\\']\\n\\n# Split the dataset into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Initialize the decision tree classifier\\nclf = DecisionTree(max_depth=5, min_samples_split=10)\\n#random_forest = RandomForest(n_estimators=100, max_depth=10, min_samples_split=10)\\n\\n# Train the classifier\\nclf.fit(X_train.values, y_train.values)  # Convert to numpy arrays\\n#random_forest.fit(X_train.values, y_train.values)\\n\\n# Make predictions on the test set\\ny_pred = clf.predict(X_test.values)  # Convert to numpy arrays\\n#y_pred_rf = random_forest.predict(X_test.values)\\n\\n# Evaluate the model\\'s performance\\naccuracy = accuracy_score(y_test, y_pred)\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nclass_report = classification_report(y_test, y_pred)\\n\\nprint(\"Accuracy:\", accuracy)\\nprint(\"Confusion Matrix:\\n\", conf_matrix)\\nprint(\"Classification Report:\\n\", class_report)\\n\\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\\nprint(f\\'Random Forest Accuracy: {accuracy_rf}\\')\\nprint(\\'Classification Report:\\')\\nprint(classification_report(y_test, y_pred_rf))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VERSION 1\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "'''\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _gini(self, y):\n",
    "        gini = 1.0\n",
    "        for i in [0,1]:\n",
    "            p = len(y[y == i]) / len(y)\n",
    "            gini -= p**2\n",
    "        return gini\n",
    "\n",
    "    def _split(self, X, y, feature, threshold):\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = X[:, feature] > threshold\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_split = None\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split(X, y, feature, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                gini_left = self._gini(y_left)\n",
    "                gini_right = self._gini(y_right)\n",
    "                gini = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {\n",
    "                        'feature': feature,\n",
    "                        'threshold': threshold,\n",
    "                        'X_left': X_left,\n",
    "                        'X_right': X_right,\n",
    "                        'y_left': y_left,\n",
    "                        'y_right': y_right\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = {\n",
    "            'depth': depth,\n",
    "            'num_samples': len(y),\n",
    "            'num_samples_per_class': num_samples_per_class,\n",
    "            'predicted_class': predicted_class,\n",
    "        }\n",
    "\n",
    "        if depth < self.max_depth and len(y) >= self.min_samples_split:\n",
    "            best_split = self._best_split(X, y)\n",
    "            if best_split:\n",
    "                node['feature'] = best_split['feature']\n",
    "                node['threshold'] = best_split['threshold']\n",
    "                node['left'] = self._build_tree(best_split['X_left'], best_split['y_left'], depth + 1)\n",
    "                node['right'] = self._build_tree(best_split['X_right'], best_split['y_right'], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if 'threshold' in node:\n",
    "            if x[node['feature']] <= node['threshold']:\n",
    "                return self._traverse_tree(x, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(x, node['right'])\n",
    "        else:\n",
    "            return node['predicted_class']\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.array([np.bincount(tree_preds[:, i]).argmax() for i in range(X.shape[0])])\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "\n",
    "# Example usage with churn dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encoding categorical features\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "    \n",
    "\n",
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "clf = DecisionTree(max_depth=5, min_samples_split=10)\n",
    "#random_forest = RandomForest(n_estimators=100, max_depth=10, min_samples_split=10)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train.values, y_train.values)  # Convert to numpy arrays\n",
    "#random_forest.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test.values)  # Convert to numpy arrays\n",
    "#y_pred_rf = random_forest.predict(X_test.values)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {accuracy_rf}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475ed03d-3215-4e62-8fc2-737d9aa5295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4003444/2507856886.py:28: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy -= p * np.log2(p)\n",
      "/tmp/ipykernel_4003444/2507856886.py:28: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  entropy -= p * np.log2(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7977288857345636\n",
      "Confusion Matrix:\n",
      " [[887 149]\n",
      " [136 237]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      1036\n",
      "           1       0.61      0.64      0.62       373\n",
      "\n",
      "    accuracy                           0.80      1409\n",
      "   macro avg       0.74      0.75      0.74      1409\n",
      "weighted avg       0.80      0.80      0.80      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Version 2.0 with entropy as well\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion  # 'gini' or 'entropy'\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _gini(self, y):\n",
    "        gini = 1.0\n",
    "        for i in [0,1]:\n",
    "            p = len(y[y == i]) / len(y)\n",
    "            gini -= p ** 2\n",
    "        return gini\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes = np.unique(y)\n",
    "        entropy = 0.0\n",
    "        for i in [0,1]:\n",
    "            p = len(y[y == i]) / len(y)\n",
    "            entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def _split(self, X, y, feature, threshold):\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = X[:, feature] > threshold\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_criterion_value = float('inf')\n",
    "        best_split = None\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split(X, y, feature, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    criterion_left = self._gini(y_left)\n",
    "                    criterion_right = self._gini(y_right)\n",
    "                elif self.criterion == 'entropy':\n",
    "                    criterion_left = self._entropy(y_left)\n",
    "                    criterion_right = self._entropy(y_right)\n",
    "\n",
    "                criterion_value = (len(y_left) / len(y)) * criterion_left + (len(y_right) / len(y)) * criterion_right\n",
    "\n",
    "                if criterion_value < best_criterion_value:\n",
    "                    best_criterion_value = criterion_value\n",
    "                    best_split = {\n",
    "                        'feature': feature,\n",
    "                        'threshold': threshold,\n",
    "                        'X_left': X_left,\n",
    "                        'X_right': X_right,\n",
    "                        'y_left': y_left,\n",
    "                        'y_right': y_right\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = {\n",
    "            'depth': depth,\n",
    "            'num_samples': len(y),\n",
    "            'num_samples_per_class': num_samples_per_class,\n",
    "            'predicted_class': predicted_class,\n",
    "        }\n",
    "\n",
    "        if depth < self.max_depth and len(y) >= self.min_samples_split:\n",
    "            best_split = self._best_split(X, y)\n",
    "            if best_split:\n",
    "                node['feature'] = best_split['feature']\n",
    "                node['threshold'] = best_split['threshold']\n",
    "                node['left'] = self._build_tree(best_split['X_left'], best_split['y_left'], depth + 1)\n",
    "                node['right'] = self._build_tree(best_split['X_right'], best_split['y_right'], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if 'threshold' in node:\n",
    "            if x[node['feature']] <= node['threshold']:\n",
    "                return self._traverse_tree(x, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(x, node['right'])\n",
    "        else:\n",
    "            return node['predicted_class']\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encoding categorical features\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "    \n",
    "\n",
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "clf = DecisionTree(max_depth=5, min_samples_split=10, criterion = 'entropy')\n",
    "#random_forest = RandomForest(n_estimators=100, max_depth=10, min_samples_split=10)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train.values, y_train.values)  # Convert to numpy arrays\n",
    "#random_forest.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test.values)  # Convert to numpy arrays\n",
    "#y_pred_rf = random_forest.predict(X_test.values)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49ea4640-7a64-41c0-8531-e93bccda89ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Anaconda3-2023.07-1/lib/python3.11/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_4003444/2507856886.py:28: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy -= p * np.log2(p)\n",
      "/tmp/ipykernel_4003444/2507856886.py:28: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  entropy -= p * np.log2(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.8119233498935415\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88      1036\n",
      "           1       0.68      0.55      0.61       373\n",
      "\n",
      "    accuracy                           0.81      1409\n",
      "   macro avg       0.76      0.73      0.74      1409\n",
      "weighted avg       0.80      0.81      0.81      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTree(max_depth=10, min_samples_split=10, criterion = 'entropy'), n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(f'Bagging Classifier Accuracy: {accuracy_bagging}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_bagging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e67f155-b57b-46ce-bb36-b6d2f3940bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7970191625266146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {accuracy_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45981b5-503d-4823-86e0-bdea312586c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055358410220014\n"
     ]
    }
   ],
   "source": [
    "#Better Random Forest\n",
    "\n",
    "model_rf2 = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,\n",
    "                                  random_state =50, \n",
    "                                   #max_features = \"auto\",\n",
    "                                  max_leaf_nodes = 30)\n",
    "model_rf2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf2 = model_rf2.predict(X_test)\n",
    "print (accuracy_score(y_test, y_pred_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c459986c-7a08-423d-8b15-20923ff31bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Accuracy: 0.794180269694819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Initialize the Extra Trees model\n",
    "extra_trees_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "extra_trees_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_et = extra_trees_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_et = accuracy_score(y_test, y_pred_et)\n",
    "print(f'Extra Trees Accuracy: {accuracy_et}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9db3e25-5d5a-42e0-8747-a6c5026a04f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Anaconda3-2023.07-1/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Classifier Accuracy: 0.7842441447835344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "# SVC does not support predict_proba by default, use a different classifier or enable probability estimates\n",
    "# svm_clf = SVC(probability=True)\n",
    "\n",
    "# Use classifiers that support predict_proba\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "\n",
    "# Create a voting classifier with soft voting\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', tree_clf), ('nb', nb_clf), ('rf', rf_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_soft = accuracy_score(y_test, y_pred_soft)\n",
    "print(f'Soft Voting Classifier Accuracy: {accuracy_soft}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "391894c9-10ea-4027-9659-a0d4d905a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Anaconda3-2023.07-1/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Classifier Accuracy: 0.7885024840312278\n"
     ]
    }
   ],
   "source": [
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', tree_clf), ('svm', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_hard = accuracy_score(y_test, y_pred_hard)\n",
    "print(f'Hard Voting Classifier Accuracy: {accuracy_hard}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
