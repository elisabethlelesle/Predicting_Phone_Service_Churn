{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca34a69-92da-401c-8439-4baf6d8c87a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass DecisionTree:\\n    def __init__(self, max_depth=None, min_samples_split=2):\\n        self.max_depth = max_depth\\n        self.min_samples_split = min_samples_split\\n        self.tree = None\\n\\n    def fit(self, X, y):\\n        self.tree = self._build_tree(X, y)\\n\\n    def predict(self, X):\\n        return np.array([self._traverse_tree(x, self.tree) for x in X])\\n\\n    def _gini(self, y):\\n        gini = 1.0\\n        for i in [0,1]:\\n            p = len(y[y == i]) / len(y)\\n            gini -= p**2\\n        return gini\\n\\n    def _split(self, X, y, feature, threshold):\\n        left_mask = X[:, feature] <= threshold\\n        right_mask = X[:, feature] > threshold\\n        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\\n\\n    def _best_split(self, X, y):\\n        best_gini = float(\\'inf\\')\\n        best_split = None\\n        for feature in range(X.shape[1]):\\n            thresholds = np.unique(X[:, feature])\\n            for threshold in thresholds:\\n                X_left, X_right, y_left, y_right = self._split(X, y, feature, threshold)\\n                if len(y_left) == 0 or len(y_right) == 0:\\n                    continue\\n\\n                gini_left = self._gini(y_left)\\n                gini_right = self._gini(y_right)\\n                gini = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right\\n\\n                if gini < best_gini:\\n                    best_gini = gini\\n                    best_split = {\\n                        \\'feature\\': feature,\\n                        \\'threshold\\': threshold,\\n                        \\'X_left\\': X_left,\\n                        \\'X_right\\': X_right,\\n                        \\'y_left\\': y_left,\\n                        \\'y_right\\': y_right\\n                    }\\n        return best_split\\n\\n    def _build_tree(self, X, y, depth=0):\\n        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\\n        predicted_class = np.argmax(num_samples_per_class)\\n        node = {\\n            \\'depth\\': depth,\\n            \\'num_samples\\': len(y),\\n            \\'num_samples_per_class\\': num_samples_per_class,\\n            \\'predicted_class\\': predicted_class,\\n        }\\n\\n        if depth < self.max_depth and len(y) >= self.min_samples_split:\\n            best_split = self._best_split(X, y)\\n            if best_split:\\n                node[\\'feature\\'] = best_split[\\'feature\\']\\n                node[\\'threshold\\'] = best_split[\\'threshold\\']\\n                node[\\'left\\'] = self._build_tree(best_split[\\'X_left\\'], best_split[\\'y_left\\'], depth + 1)\\n                node[\\'right\\'] = self._build_tree(best_split[\\'X_right\\'], best_split[\\'y_right\\'], depth + 1)\\n        return node\\n\\n    def _traverse_tree(self, x, node):\\n        if \\'threshold\\' in node:\\n            if x[node[\\'feature\\']] <= node[\\'threshold\\']:\\n                return self._traverse_tree(x, node[\\'left\\'])\\n            else:\\n                return self._traverse_tree(x, node[\\'right\\'])\\n        else:\\n            return node[\\'predicted_class\\']\\n\\nclass RandomForest:\\n    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2):\\n        self.n_estimators = n_estimators\\n        self.max_depth = max_depth\\n        self.min_samples_split = min_samples_split\\n        self.trees = []\\n\\n    def fit(self, X, y):\\n        self.trees = []\\n        for _ in range(self.n_estimators):\\n            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\\n            X_sample, y_sample = self._bootstrap_sample(X, y)\\n            tree.fit(X_sample, y_sample)\\n            self.trees.append(tree)\\n\\n    def predict(self, X):\\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\\n        return np.array([np.bincount(tree_preds[:, i]).argmax() for i in range(X.shape[0])])\\n\\n    def _bootstrap_sample(self, X, y):\\n        n_samples = X.shape[0]\\n        indices = np.random.choice(n_samples, n_samples, replace=True)\\n        return X[indices], y[indices]\\n\\n\\n# Example usage with churn dataset\\n\\n# Load the dataset\\ndf = pd.read_csv(\\'dataset.csv\\')\\ndf.dropna(inplace=True)\\n\\n# Encoding categorical features\\nlabel_encoders = {}\\nfor column in df.select_dtypes(include=[\\'object\\']).columns:\\n    label_encoders[column] = LabelEncoder()\\n    df[column] = label_encoders[column].fit_transform(df[column])\\n    \\n\\nX = df.drop(columns=[\\'Churn\\'])\\ny = df[\\'Churn\\']\\n\\n# Split the dataset into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Initialize the decision tree classifier\\nclf = DecisionTree(max_depth=5, min_samples_split=10)\\n#random_forest = RandomForest(n_estimators=100, max_depth=10, min_samples_split=10)\\n\\n# Train the classifier\\nclf.fit(X_train.values, y_train.values)  # Convert to numpy arrays\\n#random_forest.fit(X_train.values, y_train.values)\\n\\n# Make predictions on the test set\\ny_pred = clf.predict(X_test.values)  # Convert to numpy arrays\\n#y_pred_rf = random_forest.predict(X_test.values)\\n\\n# Evaluate the model\\'s performance\\naccuracy = accuracy_score(y_test, y_pred)\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nclass_report = classification_report(y_test, y_pred)\\n\\nprint(\"Accuracy:\", accuracy)\\nprint(\"Confusion Matrix:\\n\", conf_matrix)\\nprint(\"Classification Report:\\n\", class_report)\\n\\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\\nprint(f\\'Random Forest Accuracy: {accuracy_rf}\\')\\nprint(\\'Classification Report:\\')\\nprint(classification_report(y_test, y_pred_rf))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VERSION 1\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "'''\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _gini(self, y):\n",
    "        gini = 1.0\n",
    "        for i in [0,1]:\n",
    "            p = len(y[y == i]) / len(y)\n",
    "            gini -= p**2\n",
    "        return gini\n",
    "\n",
    "    def _split(self, X, y, feature, threshold):\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = X[:, feature] > threshold\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_split = None\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split(X, y, feature, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                gini_left = self._gini(y_left)\n",
    "                gini_right = self._gini(y_right)\n",
    "                gini = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {\n",
    "                        'feature': feature,\n",
    "                        'threshold': threshold,\n",
    "                        'X_left': X_left,\n",
    "                        'X_right': X_right,\n",
    "                        'y_left': y_left,\n",
    "                        'y_right': y_right\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = {\n",
    "            'depth': depth,\n",
    "            'num_samples': len(y),\n",
    "            'num_samples_per_class': num_samples_per_class,\n",
    "            'predicted_class': predicted_class,\n",
    "        }\n",
    "\n",
    "        if depth < self.max_depth and len(y) >= self.min_samples_split:\n",
    "            best_split = self._best_split(X, y)\n",
    "            if best_split:\n",
    "                node['feature'] = best_split['feature']\n",
    "                node['threshold'] = best_split['threshold']\n",
    "                node['left'] = self._build_tree(best_split['X_left'], best_split['y_left'], depth + 1)\n",
    "                node['right'] = self._build_tree(best_split['X_right'], best_split['y_right'], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if 'threshold' in node:\n",
    "            if x[node['feature']] <= node['threshold']:\n",
    "                return self._traverse_tree(x, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(x, node['right'])\n",
    "        else:\n",
    "            return node['predicted_class']\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.array([np.bincount(tree_preds[:, i]).argmax() for i in range(X.shape[0])])\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "\n",
    "# Example usage with churn dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encoding categorical features\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "    \n",
    "\n",
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "clf = DecisionTree(max_depth=5, min_samples_split=10)\n",
    "#random_forest = RandomForest(n_estimators=100, max_depth=10, min_samples_split=10)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train.values, y_train.values)  # Convert to numpy arrays\n",
    "#random_forest.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test.values)  # Convert to numpy arrays\n",
    "#y_pred_rf = random_forest.predict(X_test.values)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {accuracy_rf}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "475ed03d-3215-4e62-8fc2-737d9aa5295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3463692/3535946127.py:28: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy -= p * np.log2(p)\n",
      "/tmp/ipykernel_3463692/3535946127.py:28: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  entropy -= p * np.log2(p)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 139\u001b[0m\n\u001b[1;32m    135\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTree(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, criterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m#random_forest = RandomForest(n_estimators=100, max_depth=10, min_samples_split=10)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)  \u001b[38;5;66;03m# Convert to numpy arrays\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#random_forest.fit(X_train.values, y_train.values)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m    143\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)  \u001b[38;5;66;03m# Convert to numpy arrays\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X, y)\n",
      "Cell \u001b[0;32mIn[34], line 78\u001b[0m, in \u001b[0;36mDecisionTree._build_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     70\u001b[0m node \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: depth,\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(y),\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples_per_class\u001b[39m\u001b[38;5;124m'\u001b[39m: num_samples_per_class,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_class\u001b[39m\u001b[38;5;124m'\u001b[39m: predicted_class,\n\u001b[1;32m     75\u001b[0m }\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split:\n\u001b[0;32m---> 78\u001b[0m     best_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_split(X, y)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_split:\n\u001b[1;32m     80\u001b[0m         node[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[34], line 42\u001b[0m, in \u001b[0;36mDecisionTree._best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     40\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(X[:, feature])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m thresholds:\n\u001b[0;32m---> 42\u001b[0m     X_left, X_right, y_left, y_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X, y, feature, threshold)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_left) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_right) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 34\u001b[0m, in \u001b[0;36mDecisionTree._split\u001b[0;34m(self, X, y, feature, threshold)\u001b[0m\n\u001b[1;32m     32\u001b[0m left_mask \u001b[38;5;241m=\u001b[39m X[:, feature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m     33\u001b[0m right_mask \u001b[38;5;241m=\u001b[39m X[:, feature] \u001b[38;5;241m>\u001b[39m threshold\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
      "File \u001b[0;32m/usr/local/Anaconda3-2023.07-1/lib/python3.11/site-packages/pandas/core/series.py:1005\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[1;32m   1004\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_with(key)\n",
      "File \u001b[0;32m/usr/local/Anaconda3-2023.07-1/lib/python3.11/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_values\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, indexer: \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m-> 1069\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mgetitem_mgr(indexer)\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_mgr)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Anaconda3-2023.07-1/lib/python3.11/site-packages/pandas/core/internals/managers.py:2034\u001b[0m, in \u001b[0;36mSingleBlockManager.getitem_mgr\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   2031\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(array)))\n\u001b[1;32m   2032\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(blk)(array, placement\u001b[38;5;241m=\u001b[39mbp, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2034\u001b[0m new_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[indexer]\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;66;03m# TODO(CoW) in theory only need to track reference if new_array is a view\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m ref \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(blk)\n",
      "File \u001b[0;32m/usr/local/Anaconda3-2023.07-1/lib/python3.11/site-packages/pandas/core/indexes/base.py:5339\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5337\u001b[0m         key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m-> 5339\u001b[0m result \u001b[38;5;241m=\u001b[39m getitem(key)\n\u001b[1;32m   5340\u001b[0m \u001b[38;5;66;03m# Because we ruled out integer above, we always get an arraylike here\u001b[39;00m\n\u001b[1;32m   5341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Version 2.0 with entropy as well\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion  # 'gini' or 'entropy'\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _gini(self, y):\n",
    "        gini = 1.0\n",
    "        for i in [0,1]:\n",
    "            p = len(y[y == i]) / len(y)\n",
    "            gini -= p ** 2\n",
    "        return gini\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes = np.unique(y)\n",
    "        entropy = 0.0\n",
    "        for i in [0,1]:\n",
    "            p = len(y[y == i]) / len(y)\n",
    "            entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def _split(self, X, y, feature, threshold):\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = X[:, feature] > threshold\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_criterion_value = float('inf')\n",
    "        best_split = None\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split(X, y, feature, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    criterion_left = self._gini(y_left)\n",
    "                    criterion_right = self._gini(y_right)\n",
    "                elif self.criterion == 'entropy':\n",
    "                    criterion_left = self._entropy(y_left)\n",
    "                    criterion_right = self._entropy(y_right)\n",
    "\n",
    "                criterion_value = (len(y_left) / len(y)) * criterion_left + (len(y_right) / len(y)) * criterion_right\n",
    "\n",
    "                if criterion_value < best_criterion_value:\n",
    "                    best_criterion_value = criterion_value\n",
    "                    best_split = {\n",
    "                        'feature': feature,\n",
    "                        'threshold': threshold,\n",
    "                        'X_left': X_left,\n",
    "                        'X_right': X_right,\n",
    "                        'y_left': y_left,\n",
    "                        'y_right': y_right\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = {\n",
    "            'depth': depth,\n",
    "            'num_samples': len(y),\n",
    "            'num_samples_per_class': num_samples_per_class,\n",
    "            'predicted_class': predicted_class,\n",
    "        }\n",
    "\n",
    "        if depth < self.max_depth and len(y) >= self.min_samples_split:\n",
    "            best_split = self._best_split(X, y)\n",
    "            if best_split:\n",
    "                node['feature'] = best_split['feature']\n",
    "                node['threshold'] = best_split['threshold']\n",
    "                node['left'] = self._build_tree(best_split['X_left'], best_split['y_left'], depth + 1)\n",
    "                node['right'] = self._build_tree(best_split['X_right'], best_split['y_right'], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if 'threshold' in node:\n",
    "            if x[node['feature']] <= node['threshold']:\n",
    "                return self._traverse_tree(x, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(x, node['right'])\n",
    "        else:\n",
    "            return node['predicted_class']\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encoding categorical features\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "    \n",
    "\n",
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)  # Resample the data to balance classes\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature variables using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training and testing data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = DecisionTree(max_depth=5, min_samples_split=10, criterion = 'entropy')\n",
    "#random_forest = RandomForest(n_estimators=100, max_depth=10, min_samples_split=10)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)  # Convert to numpy arrays\n",
    "#random_forest.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)  # Convert to numpy arrays\n",
    "#y_pred_rf = random_forest.predict(X_test.values)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8646af55-f74d-41a4-b798-ab3e8df71582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy is : 0.8\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train,y_train)\n",
    "predictdt_y = dt_model.predict(X_test)\n",
    "accuracy_dt = dt_model.score(X_test,y_test)\n",
    "print(\"Decision Tree accuracy is :\",accuracy_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea4640-7a64-41c0-8531-e93bccda89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTree(max_depth=10, min_samples_split=10, criterion = 'entropy'), n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(f'Bagging Classifier Accuracy: {accuracy_bagging}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_bagging))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e67f155-b57b-46ce-bb36-b6d2f3940bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy 50: 0.8589371980676328\n",
      "Random Forest Accuracy 100: 0.8642512077294686\n",
      "Random Forest Accuracy 200: 0.8584541062801933\n",
      "Random Forest Accuracy 300: 0.8584541062801933\n",
      "Random Forest Accuracy 400: 0.8608695652173913\n",
      "Random Forest Accuracy 500: 0.8603864734299517\n",
      "Random Forest Accuracy 600: 0.8599033816425121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for i in [50,100,200,300,400,500,600]:\n",
    "    \n",
    "    random_forest_model = RandomForestClassifier(n_estimators=i, random_state=180)\n",
    "    random_forest_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_rf = random_forest_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    print(f'Random Forest Accuracy {i}: {accuracy_rf}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5095be6f-14b4-4f81-8289-b698a1694e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86      1018\n",
      "           1       0.87      0.87      0.87      1052\n",
      "\n",
      "    accuracy                           0.86      2070\n",
      "   macro avg       0.86      0.86      0.86      2070\n",
      "weighted avg       0.86      0.86      0.86      2070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_report_rf = classification_report(y_pred_rf, y_test)\n",
    "print(\"Random Forest Report:\\n\", class_report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c45981b5-503d-4823-86e0-bdea312586c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8342995169082126\n",
      "RF2 report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       990\n",
      "           1       0.85      0.83      0.84      1080\n",
      "\n",
      "    accuracy                           0.83      2070\n",
      "   macro avg       0.83      0.83      0.83      2070\n",
      "weighted avg       0.83      0.83      0.83      2070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Better Random Forest\n",
    "\n",
    "model_rf2 = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,\n",
    "                                  random_state =50, \n",
    "                                   #max_features = \"auto\",\n",
    "                                  max_leaf_nodes = 30)\n",
    "model_rf2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf2 = model_rf2.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_rf2))\n",
    "\n",
    "class_report_rf2 = classification_report(y_pred_rf2, y_test)\n",
    "print(\"RF2 report:\\n\", class_report_rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c459986c-7a08-423d-8b15-20923ff31bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Accuracy: 0.855072463768116\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      1041\n",
      "           1       0.85      0.86      0.86      1029\n",
      "\n",
      "    accuracy                           0.86      2070\n",
      "   macro avg       0.86      0.86      0.86      2070\n",
      "weighted avg       0.86      0.86      0.86      2070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Initialize the Extra Trees model\n",
    "extra_trees_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "extra_trees_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_et = extra_trees_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_et = accuracy_score(y_test, y_pred_et)\n",
    "print(f'Extra Trees Accuracy: {accuracy_et}')\n",
    "\n",
    "class_report_et = classification_report(y_pred_et, y_test)\n",
    "print(\"Extra Trees Report:\\n\", class_report_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08583e1e-5b0c-4968-8c9b-7cdf14c1a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy is : 0.851207729468599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85      1021\n",
      "           1       0.85      0.86      0.85      1049\n",
      "\n",
      "    accuracy                           0.85      2070\n",
      "   macro avg       0.85      0.85      0.85      2070\n",
      "weighted avg       0.85      0.85      0.85      2070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_model = SVC(random_state = 1)\n",
    "svc_model.fit(X_train,y_train)\n",
    "predict_y = svc_model.predict(X_test)\n",
    "accuracy_svc = svc_model.score(X_test,y_test)\n",
    "print(\"SVC accuracy is :\",accuracy_svc)\n",
    "print(classification_report(y_test, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db3e25-5d5a-42e0-8747-a6c5026a04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "# SVC does not support predict_proba by default, use a different classifier or enable probability estimates\n",
    "# svm_clf = SVC(probability=True)\n",
    "\n",
    "# Use classifiers that support predict_proba\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "\n",
    "# Create a voting classifier with soft voting\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', tree_clf), ('nb', nb_clf), ('rf', rf_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_soft = accuracy_score(y_test, y_pred_soft)\n",
    "print(f'Soft Voting Classifier Accuracy: {accuracy_soft}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391894c9-10ea-4027-9659-a0d4d905a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', tree_clf), ('svm', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_hard = accuracy_score(y_test, y_pred_hard)\n",
    "print(f'Hard Voting Classifier Accuracy: {accuracy_hard}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
